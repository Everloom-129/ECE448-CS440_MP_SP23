{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS440/ECE448 Spring 2023\n",
    "# MP01: Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to do is to download this file: <a href=\"mp01.zip\">mp01.zip</a>.  It has the following content:\n",
    "\n",
    "* `submitted.py`: Your homework. Edit, and then submit to <a href=\"https://www.gradescope.com/courses/486387\">Gradescope</a>.\n",
    "* `mp01_notebook.ipynb`: This is a <a href=\"https://anaconda.org/anaconda/jupyter\">Jupyter</a> notebook to help you debug.  You can completely ignore it if you want, although you might find that it gives you useful instructions.\n",
    "* `grade.py`: Once your homework seems to be working, you can test it by typing `python grade.py`, which will run the tests in `tests/tests_visible.py`.\n",
    "* `tests/test_visible.py`: This file contains about half of the <a href=\"https://docs.python.org/3/library/unittest.html\">unit tests</a> that Gradescope will run in order to grade your homework.  If you can get a perfect score on these tests, then you should also get a perfect score on the additional hidden tests that Gradescope uses.\n",
    "* `solution.json`: This file contains the solutions for the visible test cases, in <a href=\"https://docs.python.org/3/library/json.html\">JSON</a> format.  If the instructions are confusing you, please look at this file, to see if it can help to clear up your confusion.\n",
    "* `data`: This directory contains the data.\n",
    "* `reader.py`: This is an auxiliary program that you can use to read the data.\n",
    "* `requirements.txt`: This tells you which python packages you need to have installed, in order to run `grade.py`.  You can install all of those packages by typing `pip install -r requirements.txt` or `pip3 install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file (`mp01_notebook.ipynb`) will walk you through the whole MP, giving you instructions and debugging tips as you go.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. <a href=\"#section1\">Reading the data</a>\n",
    "1. <a href=\"#section2\">Joint and Conditional Distributions</a>\n",
    "1. <a href=\"#section3\">Mean, Variance and Covariance</a>\n",
    "1. <a href=\"#section4\">Expected Value of a Function of an RV</a>\n",
    "1. <a href=\"#grade\">Grade Your Homework</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of data: visible data (provided to you), and hidden data (available only to the autograder on Gradescope).  If you get your code working for the visible data, it should also work for the hidden data.\n",
    "\n",
    "The visible dataset consist of 500 emails, a subset of the <a href=\"https://www.kaggle.com/datasets/wanderfj/enron-spam\">Enron-Spam dataset</a> provided by Ion Androutsopoulos. MP02 will use a larger portion of the same dataset.\n",
    "\n",
    "In order to help you load the data, we provide you with a utility function called `reader.py`.  Since its methods are correctly documented by <a href=\"https://en.wikipedia.org/wiki/Docstring\">docstrings</a>, you can find information about each function by using `help`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module reader:\n",
      "\n",
      "NAME\n",
      "    reader - This file is responsible for providing functions for reading the files\n",
      "\n",
      "FUNCTIONS\n",
      "    loadDir(dirname, stemming, lower_case, use_tqdm=True)\n",
      "        Loads the files in the folder and returns a \n",
      "        list of lists of words from the text in each file.\n",
      "        \n",
      "        Parameters:\n",
      "        name (str): the directory containing the data\n",
      "        stemming (bool): if True, use NLTK's stemmer to remove suffixes\n",
      "        lower_case (bool): if True, convert letters to lowercase\n",
      "        use_tqdm (bool, default:True): if True, use tqdm to show status bar\n",
      "        \n",
      "        Output:\n",
      "        texts (list of lists): texts[m][n] is the n'th word in the m'th email\n",
      "        count (int): number of files loaded\n",
      "    \n",
      "    loadFile(filename, stemming, lower_case)\n",
      "        Load a file, and returns a list of words.\n",
      "        \n",
      "        Parameters:\n",
      "        filename (str): the directory containing the data\n",
      "        stemming (bool): if True, use NLTK's stemmer to remove suffixes\n",
      "        lower_case (bool): if True, convert letters to lowercase\n",
      "        \n",
      "        Output:\n",
      "        x (list): x[n] is the n'th word in the file\n",
      "\n",
      "DATA\n",
      "    bad_words = {'aed', 'eed', 'oed'}\n",
      "    porter_stemmer = <PorterStemmer>\n",
      "    tokenizer = RegexpTokenizer(pattern='\\\\w+', gaps=False, disc...ty=True...\n",
      "\n",
      "FILE\n",
      "    /Users/jhasegaw/Dropbox/mark/teaching/ece448/ece448labs/spring23/mp01/src/reader.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import reader\n",
    "help(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's pretty straightforward.   Let's use it to load the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 500/500 [00:00<00:00, 6554.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(reader)\n",
    "texts, count = reader.loadDir('data',False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 500 files loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"There were\",count,\"files loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first file contained the following words: ['Subject', 'done', 'new', 'sitara', 'desk', 'request', 'ref', 'cc', '20000813', 'carey', 'per', 'scott', 's', 'request', 'below', 'the', 'following', 'business', 'unit', 'aka', 'desk', 'id', 'portfolio', 'was', 'added', 'to', 'global', 'production', 'and', 'unify', 'development', 'test', 'production', 'and', 'stage', 'please', 'copy', 'to', 'the', 'other', 'global', 'environments', 'thanks', 'dick', 'x', '3', '1489', 'updated', 'in', 'global', 'production', 'environment', 'gcc', 'code', 'desc', 'p', 'ent', 'subenti', 'data', '_', 'cd', 'ap', 'data', '_', 'desc', 'code', '_', 'id', 'a', 'sit', 'deskid', 'imcl', 'a', 'ena', 'im', 'cleburne', '9273', 'from', 'scott', 'mills', '08', '30', '2000', '08', '27', 'am', 'to', 'samuel', 'schott', 'hou', 'ect', 'ect', 'richard', 'elwood', 'hou', 'ect', 'ect', 'debbie', 'r', 'brackett', 'hou', 'ect', 'ect', 'judy', 'rose', 'hou', 'ect', 'ect', 'vanessa', 'schulte', 'corp', 'enron', 'enron', 'david', 'baumbach', 'hou', 'ect', 'ect', 'daren', 'j', 'farmer', 'hou', 'ect', 'ect', 'dave', 'nommensen', 'hou', 'ect', 'ect', 'donna', 'greif', 'hou', 'ect', 'ect', 'shawna', 'johnson', 'corp', 'enron', 'enron', 'russ', 'severson', 'hou', 'ect', 'ect', 'cc', 'subject', 'new', 'sitara', 'desk', 'request', 'this', 'needs', 'to', 'be', 'available', 'in', 'production', 'by', 'early', 'afternoon', 'sorry', 'for', 'the', 'short', 'notice', 'srm', 'x', '33548']\n"
     ]
    }
   ],
   "source": [
    "print(\"The first file contained the following words:\",texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint, Conditional, and Marginal Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's MP, we will work with the following two random variables:\n",
    "\n",
    "* $X_1=$ the number of times that word1 occurs in a text\n",
    "* $X_2=$ the number of times that word2 occurs in a text\n",
    "\n",
    "... where you can specify word1 and word2 as parameters of the function.  In this section, we will compute the joint, conditional, and marginal distributions of $X_1$ and $X_2$.  These will be estimated, from the available data, using the following formulas, where $N(X_1=x_1,X_2=x_2)$ is the number of texts in the dataset that contain $x_1$ instances of word1, and $x_2$ instances of word2:\n",
    "\n",
    "#### Joint distribution:\n",
    "\n",
    "$$P(X_1=x_1,X_2=x_2)=\\frac{N(X_1=x_1,X_2=x_2)}{\\sum_{x_1}\\sum_{x_2} N(X_1=x_1,X_2=x_2)}$$\n",
    "\n",
    "#### Marginal distributions:\n",
    "\n",
    "$$P(X_1=x_1)=\\sum_{x_2} P(X_1=x_1,X_2=x_2)$$\n",
    "$$P(X_2=x_2)=\\sum_{x_1} P(X_1=x_1,X_2=x_2)$$\n",
    "\n",
    "#### Conditional distribution:\n",
    "\n",
    "$$P(X_2=x_2|X_1=x_1)=\\frac{P(X_1=x_1,X_2=x_2)}{P(X_1=x_1)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we'll load the file `submitted.py`.\n",
    "\n",
    "The file `submitted.py` is the only part of your work that the autograder will see. The only purpose of this notebook is to help you debug `submitted.py`.  Once you have revised `submitted.py` enough to make this notebook work, then you should go to the command line, and type `python grade.py`.  Once that command returns without errors, then  you can go ahead and submit your file `submitted.py` to the autograder.  You can submit to the autograder as often as you want, but it will save you trouble if you debug as much as you can on your local machine, before you submit to the autograder.\n",
    "\n",
    "We will use `importlib` in order to reload your `submitted.py` over and over again.  That way, every time you make a modification in `submitted.py`, you can just re-run  the corresponding block of this notebook, and it will reload `submitted.py` with your modified code.  \n",
    "\n",
    "Since the file is called `submitted.py`, python considers it to contain a module called `submitted`.  As shown, you can read the module's docstring by printing `submitted.__doc__`.  You can also type `help(submitted)` to get a lot of information about the module, including its docstring, a list of all the functions it defines, and all of their docstrings.  For  more about docstrings, see, for example, https://www.python.org/dev/peps/pep-0257/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is the module you'll submit to the autograder.\n",
      "\n",
      "There are several function definitions, here, that raise RuntimeErrors.  You should replace\n",
      "each \"raise RuntimeError\" line with a line that performs the function specified in the\n",
      "function's docstring.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import submitted\n",
    "import importlib\n",
    "importlib.reload(submitted)\n",
    "print(submitted.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for you to open `submitted.py`, and start editing it.  You can open it in another Jupyter window by choosing \"Open from Path\" from the \"File\" menu, and then typing `submitted.py`.  Alternatively, you can use any text editor.\n",
    "\n",
    "Once you have it open, try editing the function `joint_distribution_of_word_counts` so that its functionality matches its docstring.  Here is what it's docstring says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function joint_distribution_of_word_counts in module submitted:\n",
      "\n",
      "joint_distribution_of_word_counts(texts, word0, word1)\n",
      "    Parameters:\n",
      "    texts (list of lists) - a list of texts; each text is a list of words\n",
      "    word0 (str) - the first word to count\n",
      "    word1 (str) - the second word to count\n",
      "    \n",
      "    Output:\n",
      "    Pjoint (numpy array) - Pjoint[m,n] = P(X1=m,X2=n), where\n",
      "      X0 is the number of times that word1 occurs in a given text,\n",
      "      X1 is the number of times that word2 occurs in the same text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(submitted.joint_distribution_of_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit `joint_distribution_of_word_counts` so that it does the task specified in its docstring.  When you get the code working, you can count the number of times that the words \"Mr.\" and \"company\" co-occur.  It turns out that 96.4% of all texts contain neither word.  2.4% of texts contain the word \"company\" just once, 0.2% contain it twice, 0.2% contain it four times.  0.6% contain the word \"Mr.\" just once, 0.2% contain it four times.  There are no files in the whole database that contain both words together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.964 0.024 0.002 0.    0.002]\n",
      " [0.006 0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.   ]\n",
      " [0.002 0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "Pjoint = submitted.joint_distribution_of_word_counts(texts, 'mr', 'company')\n",
    "print(Pjoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, edit the functions `marginal_distribution_of_word_counts` and `conditional_distribution_of_word_counts`.  The results you should get are shown below, and are also available to you in the file `solutions.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.992 0.006 0.    0.    0.002]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "P0 = submitted.marginal_distribution_of_word_counts(Pjoint, 0)\n",
    "print(P0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.972 0.024 0.002 0.    0.002]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "P1 = submitted.marginal_distribution_of_word_counts(Pjoint, 1)\n",
    "print(P1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional distribution table:\n",
      "[[0.97177419 0.02419355 0.00201613 0.         0.00201613]\n",
      " [1.         0.         0.         0.         0.        ]\n",
      " [       nan        nan        nan        nan        nan]\n",
      " [       nan        nan        nan        nan        nan]\n",
      " [1.         0.         0.         0.         0.        ]]\n",
      "\n",
      "Sums of the rows:\n",
      "[ 1.  1. nan nan  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "importlib.reload(submitted)\n",
    "Pcond = submitted.conditional_distribution_of_word_counts(Pjoint, P0)\n",
    "print(\"Conditional distribution table:\")\n",
    "print(Pcond)\n",
    "print(\"\\nSums of the rows:\")\n",
    "print(np.sum(Pcond, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean, Variance and Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to study mean, variance and covariance, let's first find the joint distribution of some pair of words that occur more frequently.  How about \"a\" and \"the\"?  Amazingly, as the following code, there is a small nonzero probability that \"a\" occurs 19 times, and \"the\" occurs 58 times, in the same text!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the joint distribution:\n",
      "[[0.248 0.078 0.056 ... 0.    0.    0.   ]\n",
      " [0.036 0.028 0.026 ... 0.    0.    0.   ]\n",
      " [0.006 0.006 0.014 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.    0.    ... 0.    0.    0.   ]\n",
      " [0.    0.    0.    ... 0.    0.    0.   ]\n",
      " [0.    0.    0.    ... 0.    0.    0.002]]\n",
      "\n",
      " It has size (20, 59)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "Pathe = submitted.joint_distribution_of_word_counts(texts, 'a', 'the')\n",
    "\n",
    "print(\"Here is the joint distribution:\")\n",
    "print(Pathe)\n",
    "print(\"\\n It has size\", Pathe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of the word /the/ have the following distribution:\n",
      "[0.296 0.122 0.106 0.09  0.076 0.056 0.026 0.04  0.032 0.026 0.016 0.01\n",
      " 0.014 0.008 0.014 0.006 0.008 0.004 0.008 0.002 0.004 0.002 0.    0.002\n",
      " 0.    0.008 0.01  0.002 0.    0.006 0.    0.    0.    0.    0.    0.004\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.002]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "Pthe = submitted.marginal_distribution_of_word_counts(Pathe, 1)\n",
    "\n",
    "print(\"Counts of the word /the/ have the following distribution:\")\n",
    "print(Pthe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate mean, variance, and covariance.  First, look at their docstrings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mean_from_distribution in module submitted:\n",
      "\n",
      "mean_from_distribution(P)\n",
      "    Parameters:\n",
      "    P (numpy array) - P[n] = P(X=n)\n",
      "    \n",
      "    Outputs:\n",
      "    mu (float) - the mean of X\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.mean_from_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function variance_from_distribution in module submitted:\n",
      "\n",
      "variance_from_distribution(P)\n",
      "    Parameters:\n",
      "    P (numpy array) - P[n] = P(X=n)\n",
      "    \n",
      "    Outputs:\n",
      "    var (float) - the variance of X\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.variance_from_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function covariance_from_distribution in module submitted:\n",
      "\n",
      "covariance_from_distribution(P)\n",
      "    Parameters:\n",
      "    P (numpy array) - P[m,n] = P(X0=m,X1=n)\n",
      "    \n",
      "    Outputs:\n",
      "    covar (float) - the covariance of X0 and X1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.covariance_from_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand them, try editing `submitted.py` so that these functions perform the specified tasks.  You should get the following results (which are also provided to you in the file `solutions.json`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.432\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "mu_the = submitted.mean_from_distribution(Pthe)\n",
    "print(mu_the)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.601376\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "var_the = submitted.variance_from_distribution(Pthe)\n",
    "print(var_the)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.244752\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "covar_a_the = submitted.covariance_from_distribution(Pathe)\n",
    "print(covar_a_the)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value of a Function of an RV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the expected value of an arbitrary function of a random variable.  If $f(x_0,x_1)$ is some real-valued function of variables $x_0$ and $x_1$, then its expected value is:\n",
    "\n",
    "$$E\\left[f(X_0,X_1)\\right]=\\sum_{x_0,x_1} f(x_0,x_1) P(X_0=x_0,X_1=x_1)$$\n",
    "\n",
    "Let's read the docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function expectation_of_a_function in module submitted:\n",
      "\n",
      "expectation_of_a_function(P, f)\n",
      "    Parameters:\n",
      "    P (numpy array) - joint distribution, P[m,n] = P(X0=m,X1=n)\n",
      "    f (function) - f should be a function that takes two\n",
      "       real-valued inputs, x0 and x1.  The output, z=f(x0,x1),\n",
      "       must be a real number for all values of (x0,x1)\n",
      "       such that P(X0=x0,X1=x1) is nonzero.\n",
      "    \n",
      "    Output:\n",
      "    expected (float) - the expected value, E[f(X0,X1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "help(submitted.expectation_of_a_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function needs to produce real-valued outputs for all allowable `(x0,x1)` pairs, but otherwise, it can be as weird as we like.  For example, let's define it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(0,0) is 0.0\n",
      "f(0,15) is 2.772588722239781\n",
      "f(1,1) is 1.3862943611198906\n",
      "f(19,58) is 7.073269717459711\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def f(x0,x1):\n",
    "    return(np.log(x0+1) + np.log(x1+1))\n",
    "\n",
    "print(\"f(0,0) is\",f(0,0))\n",
    "print(\"f(0,15) is\",f(0,15))\n",
    "print(\"f(1,1) is\",f(1,1))\n",
    "print(\"f(19,58) is\",f(19,58))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7722821489053828\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(submitted)\n",
    "expected = submitted.expectation_of_a_function(Pathe, f)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grade'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade your homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've reached this point, and all of the above sections work, then you're ready to try grading your homework!  Before you submit it to Gradescope, try grading it on your own machine.  This will run some visible test cases (which you can read in `tests/test_visible.py`), and compare the results to the solutions (which you can read in `solution.json`).\n",
    "\n",
    "The exclamation point (!) tells python to run the following as a shell command.  Obviously you don't need to run the code this way -- this usage is here just to remind you that you can also, if you wish, run this command in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jhasegaw/Dropbox/mark/teaching/ece448/ece448labs/spring23/mp01/src/submitted.py:67: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Pcond[m,:] = Pjoint[m,:] / Pmarginal[m]\n",
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.076s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python grade.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got any 'E' marks, it means that your code generated some runtime errors, and you need to debug those.\n",
    "\n",
    "If you got any 'F' marks, it means that your code ran without errors, but that it generated results that are different from the solutions in `solutions.json`.  Try debugging those differences.\n",
    "\n",
    "If neither of those things happened, and your result was a series of dots, then your code works perfectly.  \n",
    "\n",
    "If you're not sure, you can try running grade.py with the -j option.  This will produce a JSON results file, in which the best score you can get is 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"tests\": [\r\n",
      "        {\r\n",
      "            \"name\": \"test_cond (test_visible.TestStep)\",\r\n",
      "            \"score\": 8,\r\n",
      "            \"max_score\": 8,\r\n",
      "            \"output\": \"\\n/Users/jhasegaw/Dropbox/mark/teaching/ece448/ece448labs/spring23/mp01/src/submitted.py:67: RuntimeWarning: invalid value encountered in true_divide\\n  Pcond[m,:] = Pjoint[m,:] / Pmarginal[m]\\n\"\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"test_covariance (test_visible.TestStep)\",\r\n",
      "            \"score\": 8,\r\n",
      "            \"max_score\": 8\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"test_expected (test_visible.TestStep)\",\r\n",
      "            \"score\": 8,\r\n",
      "            \"max_score\": 8\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"test_joint (test_visible.TestStep)\",\r\n",
      "            \"score\": 9,\r\n",
      "            \"max_score\": 9\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"test_marginal (test_visible.TestStep)\",\r\n",
      "            \"score\": 9,\r\n",
      "            \"max_score\": 9\r\n",
      "        },\r\n",
      "        {\r\n",
      "            \"name\": \"test_mean (test_visible.TestStep)\",\r\n",
      "            \"score\": 8,\r\n",
      "            \"max_score\": 8\r\n",
      "        }\r\n",
      "    ],\r\n",
      "    \"leaderboard\": [],\r\n",
      "    \"visibility\": \"visible\",\r\n",
      "    \"execution_time\": \"0.06\",\r\n",
      "    \"score\": 50\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!python grade.py -j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should try uploading `submitted.py` to <a href=\"https://www.gradescope.com/courses/486387\">Gradescope</a>.  \n",
    "\n",
    "Gradescope will run the same visible tests that you just ran on your own machine, plus some additional hidden tests.  It's possible that your code passes all the visible tests, but fails the hidden tests.  If that happens, then it probably means that you hard-coded a number into your function definition, instead of using the input parameter that you were supposed to use.  Debug by running your function with a variety of different input parameters, and see if you can get it to respond correctly in all cases.\n",
    "\n",
    "Once your code works perfectly on Gradescope, with no errors, then you are done with the MP.  Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
