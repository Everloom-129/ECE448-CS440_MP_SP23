{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS440/ECE448 Spring 2023\n",
    "# MP09: Perception"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to do is to download this file: <a href=\"mp09.zip\">mp09.zip</a>.  It has the following content:\n",
    "\n",
    "* `submitted.py`: Your homework. Edit, and then submit to <a href=\"https://www.gradescope.com/courses/486387\">Gradescope</a>.\n",
    "* `mp09_notebook.ipynb`: This is a <a href=\"https://anaconda.org/anaconda/jupyter\">Jupyter</a> notebook to help you debug.  You can completely ignore it if you want, although you might find that it gives you useful instructions.\n",
    "* `tests`: This directory contains visible test.\n",
    "\n",
    "<!-- \n",
    "You will then need to download the CIFAR-10 dataset from the following link: <a href=\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\">CIFAR10 Dataset Download</a>. Please uncompress the zip file inside your mp09 folder. -->\n",
    "\n",
    "The list of modules you will need to import/install:\n",
    "\n",
    "* `torch`\n",
    "* `numpy`\n",
    "* `os`\n",
    "* `torchvision`\n",
    "\n",
    "\n",
    "You will not require a GPU for this MP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file (`mp09_notebook.ipynb`) will walk you through the full MPs with instructions and suggestion, and it is highly recommended that you follow this notebook.\n",
    "\n",
    "### Goal\n",
    "\n",
    "The objective of this assignment is to create a full end-to-end training and testing pipeline for a convolutional neural network (CNN) for the task of image classification on a modified version of the standard vision dataset `CIFAR10`. You will learn the concept of finetuning your model, in which you freeze your convolutional backbone and finetune newly initialized linear layer(s) for a specific task.\n",
    "\n",
    "There are 8 target categories: `airplane (0), automobile (1), bird (2), deer (3), frog (4), horse (5), ship (6), truck (7)`. Given an image, your CNN will be expected to successfully classify an image into one of these categories. \n",
    "\n",
    "You will be using `PyTorch` for this MP. In `MP04`, you gained some familiarity with the PyTorch library and you will build upon this foundation in this MP by designing and implementing the whole pipeline from scratch.\n",
    "\n",
    "You will need to consult the PyTorch documentation, to help you with implementation details. Please make sure you read the function definitions and descriptions in `submitted.py` carefully before completing them. \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. <a href=\"#section1\">Dataset</a>\n",
    "1. <a href=\"#section1\">Dataloader</a>\n",
    "1. <a href=\"#section3\">Model Finetuning</a>\n",
    "1. <a href=\"#grade\">Grade Your Homework</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "<a id='section2'></a>\n",
    "## Dataset\n",
    "\n",
    "In this section you will create a PyTorch Dataset based on the `torch.utils.data.Dataset` class. \n",
    "\n",
    "Some useful resources:\n",
    "\n",
    "* <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR10 Specifications</a> for understanding the data format\n",
    "* <a href=\"https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\">Datasets and Dataloaders</a> tutorial in PyTorch\n",
    "\n",
    "It is highly recommended to read and understand these resources before diving into the code. Here is a short summary:\n",
    "\n",
    "### CIFAR10 Specifications\n",
    "\n",
    "T- he data folder contains 6 files, each of which is a pickled Python object: `data_batch_1`, `data_batch_2`, `data_batch_3`, `data_batch_4`, `data_batch_5`, `test_batch`. \n",
    "\n",
    "- Each file contains roughly (this amount as we only observe 8 classes) 8000 samples from the dataset; the first 5 files correspond to our training set and the `test_batch` file corresponds to our test set. \n",
    "\n",
    "- You will need to read all of these files and import the samples in the code. There are memory-efficient ways to do so, but simply reading all the samples from the files at once (according to train/test mode) will work fine for this assignment. \n",
    "\n",
    "- Each file is a dictionary containing the data and the labels, which will be \"visible\" after unpickling. \n",
    "   - `data` is a numpy array of shape (num_samples, 3072) containing the pixel values (in range [0...255]) for the 32x32 image. \n",
    "   - The pixel values are stored in a specific order (R values, G values and then B values in row-major order) described in detail in the provided link above.\n",
    "   - `labels` is a numpy array of shape (num_samples), containing the categorical label for each sample.\n",
    "\n",
    "### Datasets and Dataloaders\n",
    "\n",
    "The `Dataset` you will need to create has three major member functions: `__init__`, `__len__`, and `__getitem__`. `__init__` is the constructor for the class inheriting `Dataset` - this is where you may want to load the data from the provided data files and store it in some member variable. `__len__` returns the length of the dataset (the number of samples) and provides the `DataLoader` wrapper with an idea of the range for index sampling. `__getitem__` should return an image and a label (a single sample) when called with a given numerical index - this function will be called many times when a batch is constructed by the `DataLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cpu\n",
      "0.15.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to visualize images from https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "# names = {0: \"ship\", 1: \"automobile\", 2: \"dog\", 3: \"frog\", 4: \"horse\"}\n",
    "# names = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"}\n",
    "names = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"deer\", 4: \"frog\", 5: \"horse\", 6: \"ship\", 7: \"truck\"}\n",
    "\n",
    "def show(imgs, figsize=None):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "\n",
    "    if figsize is not None:\n",
    "        fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "    else:\n",
    "        fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        # img = img.detach()\n",
    "        # img = F.to_pil_image(img)\n",
    "        # axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys([b'batch_label', b'data', b'labels', b'filenames'])\n",
      "b'batch_label' <class 'bytes'>\n",
      "b'data' <class 'list'>\n",
      "b'labels' <class 'list'>\n",
      "b'filenames' <class 'list'>\n",
      "Labels: {0, 1, 2, 3, 4, 5, 6, 7}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8047, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import submitted\n",
    "import importlib\n",
    "importlib.reload(submitted)\n",
    "\n",
    "file = r\"cifar10_batches/data_batch_1\"\n",
    "data_batch_1 = submitted.unpickle(file)\n",
    "\n",
    "print(type(data_batch_1))\n",
    "print(data_batch_1.keys())\n",
    "for item in data_batch_1:\n",
    "    print(item, type(data_batch_1[item]))\n",
    "print(\"Labels:\", set(data_batch_1[b'labels']))\n",
    "X_train = np.array( data_batch_1[b'data'])\n",
    "images = X_train.reshape(len(X_train),3,32,32).transpose(0,2,3,1)\n",
    "images.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some images from one of the provided data batch files and try instantiating our data loader to see whether it works. Note that you need to complete the `build_dataset` and `build_dataloader` functions, as well as the `CIFAR10` dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import submitted\n",
    "import importlib\n",
    "from torchvision import transforms\n",
    "importlib.reload(submitted)\n",
    "# it make take a little while to build the dataset \n",
    "example_dataset = submitted.build_dataset([\"cifar10_batches/data_batch_1\"], transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a automobile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "image, label = example_dataset[random.randint(0, len(example_dataset)-1)]\n",
    "print(\"An example of a\", names[label])\n",
    "image.shape\n",
    "# show(image)\n",
    "# def show_alternative(img):\n",
    "#     img = np.transpose(img, (1, 2, 0))\n",
    "#     plt.imshow(img)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.show()\n",
    "\n",
    "# show_alternative(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented your dataset class correctly, as well as your `build_dataset` function. You should be able to visualize random images from the dataset through the cell above. Feel free to run that cell as many times to verify that your dataset is working correctly. Now, let's instantiate a DataLoader so that we can sample batches of images from the dataset and visualize these images. Try visualizing multiple different batches. As a quick exercise, try printing out the labels of each element in the batch and verify whether it corresponds to the images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_params = {\"batch_size\": 4, \"shuffle\": True}\n",
    "example_dataloader = submitted.build_dataloader(example_dataset, loader_params=loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image batch shape:  torch.Size([4, 3, 32, 32])\n",
      "label batch shape:  torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "image_batch, label_batch = next(iter(example_dataloader))\n",
    "print(\"image batch shape: \", image_batch.shape)\n",
    "print(\"label batch shape: \", label_batch.shape)\n",
    "# show(make_grid([image_batch[i, :, :, :] for i in range(image_batch.shape[0])], nrows=4), figsize=(32, 32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Model Finetuning\n",
    "\n",
    "In this section you will create your own model based on a pretrained backbone and finetune it on the CIFAR10 dataset. \n",
    "\n",
    "Some useful resources:\n",
    "\n",
    "* <a href=\"https://pytorch.org/tutorials/beginner/saving_loading_models.html\">Saving and loading models</a> tutorial in PyTorch\n",
    "* <a href=\"https://pytorch.org/docs/master/notes/autograd.html#locally-disabling-gradient-computation\">Autograd mechanics</a> for understanding how to freeze parts of the model (go to section -- Locally disabling gradient computation)\n",
    "\n",
    "\n",
    "It is highly recommended to read and understand these resources before diving into the code. Here is a short summary:\n",
    "\n",
    "\n",
    "### Saving and loading models\n",
    "\n",
    "It is a very common and useful practice to leverage pretrained models (models that have been trained for a specific task already) on other downstream tasks. There can be many reasons to do so: (1) the representations learned by a model for a different dataset/task may transfer well to our desired task, (2) we may want to train the model with less compute resources so we don't want to train the entire model, etc. Intuitively, suppose you have trained a model on the massive ImageNet dataset to recognize all kinds of different objects and you obtain relatively good performance. The features the model has learned to extract through its convolutional backbone to detect \"general\" objects could be very applicable to a different dataset (with different objects). In this part of the MP, you will leverage a pretrained model and finetune it on our CIFAR10 dataset. We provide a model checkpoint `resnet18.pt` in the source code, corresponding to a pretrained version of the <a href=\"https://pytorch.org/hub/pytorch_vision_resnet/\">ResNet18</a> architecture. You will need to figure out how to load this model checkpoint and then identify which layers to use as your backbone (hint: only the final part of your network should be excluded from your backbone). \n",
    "\n",
    "After you load your model checkpoint, you need to also initialize new classification layers on top of the backbone. Think about what type of layers you would need for classification. Finally, you can complete your `forward` function in your network according to your logical separation of backbone/classifier. \n",
    "\n",
    "\n",
    "### Autograd mechanics\n",
    "\n",
    "You are almost done with your implementation of your network. The final step is to ensure that your backbone parameters are frozen! This means that the weight parameters in your backbone should not receive any gradient updates during backpropagation. In essence, we are assuming the backbone will already be effective for the task at hand (so it no longer needs to be trained) and we only the train the classifier. Refer to the PyTorch tutorial for help (there are also many online resources that discuss this topic).\n",
    "\n",
    "You are now ready to train and test your model! Fill out the `train` and `test` functions -- you should have a good reference in MP04 how to do so. Please be careful about the loss function / computation (e.g. if you use negative log likelihood loss, make sure your logits are normalized). `run_model` should orchestrate the entire training and testing flow and should call the functions you have completed so far.\n",
    "\n",
    "Note: ResNet18 is fairly large to train on a CPU. Do not be alarmed if it takes a few minutes to train and test. Through empirical verification, you should be able to get ~90% performance after 5-7 minutes of training if done correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\everloom\\AppData\\Local\\Temp\\ipykernel_5680\\1404751565.py\", line 4, in <module>\n",
      "    submitted.run_model()\n",
      "  File \"d:\\# Courses\\## 大三\\SP23\\CS440\\mp09\\submitted.py\", line 337, in run_model\n",
      "    optimizer = build_optimizer(\"SGD\", model.parameters(), {'lr': 0.002})\n",
      "  File \"d:\\# Courses\\## 大三\\SP23\\CS440\\mp09\\submitted.py\", line 231, in build_optimizer\n",
      "    optimizer = torch.optim.SGD(params = model_params, **hparams)\n",
      "  File \"C:\\Users\\everloom\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\sgd.py\", line 27, in __init__\n",
      "    super().__init__(params, defaults)\n",
      "  File \"C:\\Users\\everloom\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py\", line 187, in __init__\n",
      "    raise ValueError(\"optimizer got an empty parameter list\")\n",
      "ValueError: optimizer got an empty parameter list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\ProgramData\\Anaconda3\\envs\\ece448\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import submitted\n",
    "import importlib\n",
    "importlib.reload(submitted)\n",
    "submitted.run_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grade'></a>\n",
    "## Grade your homework\n",
    "\n",
    "If you've reached this point, and all of the above sections work, then you're ready to try grading your homework! Before you submit it to Gradescope, try grading it on your own machine. This will run some visible test cases. Note that these visible test cases do not test the accuracy of your model, but we expect your finetuned model to achieve at least 90% accuracy on the test set to pass the hidden test cases. Make sure you test locally and can ensure that you can achieve 90% accuracy before submitting to the autograder (it may take even longer to run on the autograder).\n",
    "\n",
    "The exclamation point (!) tells python to run the following as a shell command. Obviously you don't need to run the code this way -- this usage is here just to remind you that you can also, if you wish, run this command in a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gradescope_utils in c:\\users\\everloom\\appdata\\roaming\\python\\python39\\site-packages (0.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.086s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gradescope_utils\n",
    "!python grade.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94986e3e9dbe739db2cb74b7552f89a7fbb64b2040103e68447b37ec3db13c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
